# ccBench - Coding Characteristics Benchmarking Suite for Agentic Coding Tools

ccBench is a benchmarking suite designed to evaluate the performance of various agentic coding tools and configurations on solving various tasks. It provides a standardized framework to test and compare how effectively different tooling variants can generate solutions to predefined tasks.

## Prerequisites

- `uv` - A virtual environment manager. Install it from [uv's GitHub repository](https://github.com/astral-sh/uv).
- `cloc` - A tool to count lines of code. Install it via your package manager (e.g., `brew install cloc` on macOS).

## Setup

1. Clone the repository:

   ```bash
   git clone https://github.com/jannismain/ccBench.git
   cd ccBench
   ```

2. Fill in required secrets.

   Some configurations require API keys or other secrets to function properly.
   For example, if you are using the `portkey_for_claude_code` configuration, you need to set up the `.env` file with your Portkey API key.

    ```bash
    cd config_forge/portkey_for_claude_code
    cp .env.sample .env
    # Edit .env to add your Portkey API key
    ```

## Running Experiments

To run an experiment, use the following command:

```bash
uv run ccBench.py example.yaml
```

## Available Evaluations

ccBench includes several evaluation tools to analyze the results of experiments:

### `cloc` - Code Line Counter

Counts the lines of code generated by the agent, excluding the initial files. This helps measure the size and complexity of the generated solution.

**Output:** `cloc.json` - Contains line counts by language and file type.

### `claude_code_metrics` - Claude Code Performance Metrics

Extracts detailed performance metrics from Claude Code's `output.json` file, including:

- **Duration metrics:** Total execution time (ms) and API time (ms)
- **Token usage:** Input tokens, cache creation/read tokens, output tokens
- **Cost:** Total cost in USD
- **Turns:** Number of conversation turns
- **Model usage:** Breakdown by model (Sonnet, Haiku, etc.) with individual costs
- **Permission denials:** Count of permission denials during execution
- **Web search requests:** Number of web searches performed

**Output:** `claude_code_metrics.json` - Contains all extracted metrics in JSON format.
